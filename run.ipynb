{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Moudles and Packages\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"axes.grid\"]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data as D\n",
    "\n",
    "from pytorch_toolbelt import losses\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.10.0+cu102\n",
      "GPU: True\n",
      "Device name:  Tesla T4\n",
      "Device count:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"GPU: {}\".format(torch.cuda.is_available()))\n",
    "\n",
    "print(\"Device name: \", torch.cuda.get_device_name(0))\n",
    "print(\"Device count: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local modules\n",
    "from src import config as C\n",
    "from src.models import hrnet, resnet, basenet\n",
    "from src.dataset import kface_dataset as K\n",
    "from src.dataset import w300_dataset as W\n",
    "from src.dataset import kface_transform as T\n",
    "from src.utils.collate_fn import *\n",
    "from src.utils.print_overwrite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Device : cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"| Device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Number of image : 100\n",
      "| Number of label : 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"| Number of image : {len(C.IMAGE_LIST)}\")\n",
    "print(f\"| Number of label : {len(C.LABEL_LIST)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Number of train : 90\n",
      "| Number of valid : 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"| Number of train : {C.LEN_TRAIN_SET}\")\n",
    "print(f\"| Number of valid : {C.LEN_VALID_SET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dataset = K.kfacedataset(image_list=C.IMAGE_LIST, label_list=C.LABEL_LIST, transform=T.Transforms())\n",
    "w_dataset = W.FaceLandmarksDataset(W.Transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224])\n",
      "torch.Size([27, 2])\n"
     ]
    }
   ],
   "source": [
    "image, label = w_dataset[0]\n",
    "print(image.size())\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 2])\n"
     ]
    }
   ],
   "source": [
    "landmarks = label.view([label.size(0),-1])\n",
    "print(landmarks.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Train set is 6000\n",
      "The length of Valid set is 666\n",
      "Valid Steps: 333/333  Loss: 0.0031 \n",
      "--------------------------------------------------\n",
      "Epoch: 1  Train Loss: 0.0158  Valid Loss: 0.0031\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0031 at epoch 1/40\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 333/333  Loss: 0.0017 \n",
      "--------------------------------------------------\n",
      "Epoch: 2  Train Loss: 0.0038  Valid Loss: 0.0017\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0017 at epoch 2/40\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 333/333  Loss: 0.0012 \n",
      "--------------------------------------------------\n",
      "Epoch: 3  Train Loss: 0.0027  Valid Loss: 0.0012\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0012 at epoch 3/40\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 333/333  Loss: 0.0015 \n",
      "--------------------------------------------------\n",
      "Epoch: 4  Train Loss: 0.0021  Valid Loss: 0.0015\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0008 \n",
      "--------------------------------------------------\n",
      "Epoch: 5  Train Loss: 0.0019  Valid Loss: 0.0008\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0008 at epoch 5/40\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 333/333  Loss: 0.0009 \n",
      "--------------------------------------------------\n",
      "Epoch: 6  Train Loss: 0.0017  Valid Loss: 0.0009\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0010 \n",
      "--------------------------------------------------\n",
      "Epoch: 7  Train Loss: 0.0017  Valid Loss: 0.0010\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0010 \n",
      "--------------------------------------------------\n",
      "Epoch: 8  Train Loss: 0.0014  Valid Loss: 0.0010\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0009 \n",
      "--------------------------------------------------\n",
      "Epoch: 9  Train Loss: 0.0014  Valid Loss: 0.0009\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0008 \n",
      "--------------------------------------------------\n",
      "Epoch: 10  Train Loss: 0.0013  Valid Loss: 0.0008\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0011 \n",
      "--------------------------------------------------\n",
      "Epoch: 11  Train Loss: 0.0012  Valid Loss: 0.0011\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0007 \n",
      "--------------------------------------------------\n",
      "Epoch: 12  Train Loss: 0.0012  Valid Loss: 0.0007\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0007 at epoch 12/40\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 333/333  Loss: 0.0007 \n",
      "--------------------------------------------------\n",
      "Epoch: 13  Train Loss: 0.0011  Valid Loss: 0.0007\n",
      "--------------------------------------------------\n",
      "Valid Steps: 333/333  Loss: 0.0008 \n",
      "--------------------------------------------------\n",
      "Epoch: 14  Train Loss: 0.0012  Valid Loss: 0.0008\n",
      "--------------------------------------------------\n",
      "Train Steps: 27/94  Loss: 0.0011 "
     ]
    }
   ],
   "source": [
    "## 300w\n",
    "\n",
    "# split the dataset into validation and test sets\n",
    "len_valid_set = int(0.1*len(w_dataset))\n",
    "len_train_set = len(w_dataset) - len_valid_set\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_set))\n",
    "\n",
    "train_dataset , valid_dataset,  = torch.utils.data.random_split(w_dataset , [len_train_set, len_valid_set])\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# network = basenet.BaseNet()\n",
    "network = resnet.Network()\n",
    "network.train()\n",
    "network.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = MADGRAD(params=network.parameters(), lr=C.LEARNING_RATE, weight_decay=C.WEIGHT_DECAY)\n",
    "\n",
    "loss_min = np.inf\n",
    "num_epochs = 40\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "    \n",
    "        images, landmarks = next(iter(train_loader))\n",
    "        \n",
    "        images = images.to(device)\n",
    "        landmarks = landmarks.view(landmarks.size(0),-1).to(device)\n",
    "        \n",
    "        predictions = network(images)\n",
    "        \n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # find the loss for the current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "        \n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).to(device)\n",
    "        \n",
    "            predictions = network(images)\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), '/home/ubuntu/workspace/FLD-scratch/result/face_landmarks.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "     \n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    best_network = resnet.Network()\n",
    "    best_network.to(device)\n",
    "    best_network.load_state_dict(torch.load('/home/ubuntu/workspace/FLD-scratch/result/face_landmarks.pth')) \n",
    "    best_network.eval()\n",
    "    \n",
    "    images, landmarks = next(iter(valid_loader))\n",
    "    \n",
    "    images = images.to(device)\n",
    "    landmarks = (landmarks + 0.5) * 224\n",
    "\n",
    "    predictions = (best_network(images).cpu() + 0.5) * 224\n",
    "    predictions = predictions.view(-1,27,2)\n",
    "    \n",
    "    plt.figure(figsize=(10,40))\n",
    "    \n",
    "    for img_num in range(8):\n",
    "        plt.subplot(8,1,img_num+1)\n",
    "        plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray')\n",
    "        plt.scatter(predictions[img_num,:,0], predictions[img_num,:,1], c = 'r', s = 5)\n",
    "        plt.scatter(landmarks[img_num,:,0], landmarks[img_num,:,1], c = 'g', s = 5)\n",
    "\n",
    "print('Total number of test images: {}'.format(len(valid_dataset)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed Time : {}\".format(end_time - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "import imutils\n",
    "import random\n",
    "from math import *\n",
    "from PIL import Image\n",
    "\n",
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # def rotate(self, image, landmarks, angle):\n",
    "    #     angle = random.uniform(-angle, +angle)\n",
    "\n",
    "    #     transformation_matrix = torch.tensor([\n",
    "    #         [+cos(radians(angle)), -sin(radians(angle))], \n",
    "    #         [+sin(radians(angle)), +cos(radians(angle))]\n",
    "    #     ])\n",
    "\n",
    "    #     image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "    #     landmarks = landmarks - 0.5\n",
    "    #     new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "    #     new_landmarks = new_landmarks + 0.5\n",
    "    #     return Image.fromarray(image), new_landmarks\n",
    "    # def add_noise(self, images):\n",
    "    #     images = np.array(images)\n",
    "    #     noise = 0.008 * np.random.randn(224,224,1)\n",
    "    #     noise = noise.astype(np.float32)\n",
    "    #     # 생성한 noise를 원본에 add\n",
    "    #     noisy_image = cv2.add(images, noise)\n",
    "    #     return noisy_image\n",
    "    \n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "\n",
    "    # def crop_face(self, image, landmarks):\n",
    "\n",
    "    #     img_shape = np.array(image).shape\n",
    "    #     # landmarks = torch.tensor(landmarks) - torch.tensor([img_shape[1], img_shape[0]])\n",
    "    #     landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "    #     return image, landmarks\n",
    "\n",
    "    def color_jitter(self, image, landmarks):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks   \n",
    "\n",
    "\n",
    "    def __call__(self, image, landmarks):\n",
    "        image = Image.fromarray(image)\n",
    "        landmarks = torch.tensor(landmarks)\n",
    "        \n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        # image, landmarks = self.crop_face(image, landmarks)\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        # image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "        # image = self.add_noise(image)\n",
    "        # image = Image.fromarray(image)\n",
    "        # image = A.add_no\n",
    "        landmarks = landmarks * torch.tensor((1/874, 1/576))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import kface_dataset as K\n",
    "dataset = K.kfacedataset(image_list=C.IMAGE_LIST, label_list=C.LABEL_LIST, transform=Transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching Test\n",
    "image, landmarks = dataset[0]\n",
    "img_shape = np.array(image).shape\n",
    "landmark = landmarks * torch.tensor((224,224))\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image.permute(1,2,0), cmap='gray');\n",
    "plt.scatter(landmark.T[0], landmark.T[1], s=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = D.random_split(dataset, [C.LEN_TRAIN_SET, C.LEN_VALID_SET])\n",
    "\n",
    "train_loader = D.DataLoader(train_dataset, batch_size=C.BATCH_SIZE[\"TRAIN\"], shuffle=True, num_workers=4)\n",
    "valid_loader = D.DataLoader(valid_dataset, batch_size=C.BATCH_SIZE[\"VALID\"], shuffle=False, num_workers=4)\n",
    "\n",
    "train_images, train_landmarks = next(iter(train_loader))\n",
    "valid_images, valid_landmarks = next(iter(valid_loader))\n",
    "\n",
    "print(f\"| Size of image in train_loader : {train_images.shape}\")\n",
    "print(f\"| Size of label in train_loader : {train_landmarks.shape}\")\n",
    "print(f\"| Size of image in train_loader : {valid_images.shape}\")\n",
    "print(f\"| Size of label in train_loader : {valid_landmarks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# model=models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet.Network()\n",
    "model = model.cpu()\n",
    "model.load_state_dict(torch.load(\"/home/ubuntu/workspace/FLD-scratch/result/face_landmarks.pth\",map_location='cpu'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x = torch.randn([1, 1, 224, 224]).to(device)\n",
    "out = model(x).to(device)\n",
    "print(f\"input : {x.shape} | output : {out.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = MADGRAD(params=model.parameters(), lr=C.LEARNING_RATE, weight_decay=C.WEIGHT_DECAY)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=C.EPOCHS, T_mult=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min = np.inf\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,201):\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "        model = model.to(device)    \n",
    "        \n",
    "        images, landmarks = next(iter(train_loader))\n",
    "        \n",
    "        images = images.to(device)\n",
    "        landmarks = landmarks.view([landmarks.size(0),-1]).to(device)\n",
    "        \n",
    "        predictions = model(images)\n",
    "        \n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # find the loss for the current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "      \n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.view([landmarks.size(0),-1]).to(device)\n",
    "        \n",
    "            predictions = model(images).to(device)\n",
    "                    \n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "\n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('\\n--------------------------------------------------')\n",
    "    \n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(model.state_dict(), '/home/ubuntu/workspace/FLD-scratch/result/face_landmarks_2.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, C.EPOCHS))\n",
    "        print('Model Saved\\n')\n",
    "\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    best_network = resnet.Network()\n",
    "    best_network.to(device)\n",
    "    best_network.load_state_dict(torch.load('/home/ubuntu/workspace/FLD-scratch/result/face_landmarks_2.pth')) \n",
    "    best_network.eval()\n",
    "    \n",
    "    images, landmarks = next(iter(valid_loader))\n",
    "    \n",
    "    images = images.to(device)\n",
    "    landmarks = landmarks * torch.tensor((224,224))    \n",
    "\n",
    "    predictions = best_network(images).cpu()\n",
    "    predictions = predictions.view([-1,27,2])\n",
    "    predictions = predictions * torch.tensor((224,224))    \n",
    "    \n",
    "    plt.figure(figsize=(10,20))\n",
    "    \n",
    "    for img_num in range(2):\n",
    "        plt.subplot(2,1,img_num+1)\n",
    "        plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray')\n",
    "        plt.scatter(predictions[img_num].T[0], predictions[img_num].T[1], c = 'r', s = 5)\n",
    "        plt.scatter(landmarks[img_num].T[0], landmarks[img_num].T[1], c = 'g', s = 5)\n",
    "\n",
    "print('Total number of test images: {}'.format(len(valid_dataset)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed Time : {}\".format(end_time - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('FG-GLD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9550210be859de316fbaaac654d2fc4c7fb8b5570e54a571b0ab8123deb4b39d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
