{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Moudles and Packages\n",
    "import gc\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"axes.grid\"]=False\n",
    "\n",
    "# Import pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data as D\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "\n",
    "\n",
    "print(\"| Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"| GPU: {}\".format(torch.cuda.is_available()))\n",
    "print(\"| Device : \",device)\n",
    "print(\"| Device name: \", torch.cuda.get_device_name(0))\n",
    "print(\"| Device count: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local modules\n",
    "from src import config as C\n",
    "from src.models import hrnet, resnet, basenet\n",
    "\n",
    "from src.utils.collate_fn import *\n",
    "from src.utils.print_overwrite import *\n",
    "from src.utils.seed import *\n",
    "from src.utils.view_kypoints import *\n",
    "\n",
    "from src.dataset.kface_dataset import *\n",
    "from src.dataset.album_transform import *\n",
    "\n",
    "seed_everything(C.SEED)\n",
    "\n",
    "print(f\"| Number of image : {len(C.IMAGE_LIST)}\")\n",
    "print(f\"| Number of label : {len(C.LABEL_LIST)}\")\n",
    "print(f\"| Number of trainset : {C.LEN_TRAIN_SET}\")\n",
    "print(f\"| Number of validset : {C.LEN_VALID_SET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.read_csv(C.LABEL_LIST[0]).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dataset_train = kfacedataset(\n",
    "    image_list=C.IMAGE_LIST,\n",
    "    label_list=C.LABEL_LIST,\n",
    "    type=\"train\",\n",
    "    transform=get_augmentation(data_type=\"train\")\n",
    "    )\n",
    "\n",
    "k_dataset_valid = kfacedataset(\n",
    "    image_list=C.IMAGE_LIST,\n",
    "    label_list=C.LABEL_LIST,\n",
    "    type=\"valid\",\n",
    "    transform=get_augmentation(data_type=\"valid\")\n",
    "    )\n",
    "\n",
    "# # (Option) visualize dataset\n",
    "for idx, (image, landmarks) in enumerate(k_dataset_train): # torch.Size([1, 512, 512])\n",
    "    if idx == 1:\n",
    "        vis_keypoints(image, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, _ = D.random_split(k_dataset_train, [C.LEN_TRAIN_SET, C.LEN_VALID_SET])\n",
    "_, valid_dataset = D.random_split(k_dataset_valid, [C.LEN_TRAIN_SET, C.LEN_VALID_SET])\n",
    "\n",
    "train_loader = D.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "valid_loader = D.DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "train_images, train_landmarks = next(iter(train_loader))\n",
    "valid_images, valid_landmarks = next(iter(valid_loader))\n",
    "\n",
    "print(f\"| Size of image in train_loader : {train_images.shape}\")\n",
    "print(f\"| Size of label in train_loader : {train_landmarks.shape}\")\n",
    "print(f\"| Size of image in train_loader : {valid_images.shape}\")\n",
    "print(f\"| Size of label in train_loader : {valid_landmarks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class resnet18(nn.Module):\n",
    "    def __init__(self,num_classes=54):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet18'\n",
    "        self.model=models.resnet18(pretrained=True)\n",
    "    \n",
    "        self.model.conv1=nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=3, bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '/home/ubuntu/workspace/FLD-scratch/result/face_landmarks.pth'\n",
    "model = resnet18().cpu()\n",
    "if pretrained_model:\n",
    "    state_dict = torch.load(pretrained_model,map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([1, 1, 512, 512]).to(device)\n",
    "out = model(x).to(device)\n",
    "print(f\"input : {x.shape} | output : {out.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = MADGRAD(params=model.parameters(), lr=C.LEARNING_RATE, weight_decay=C.WEIGHT_DECAY)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=C.EPOCHS, T_mult=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "loss_min = np.inf\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(C.EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    scaler = GradScaler()    \n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (images, landmarks) in pbar:\n",
    "\n",
    "        landmarks = landmarks.view(landmarks.size(0),-1)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        landmarks = landmarks.to(device)\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "            model = model.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss_train_step = criterion(predictions, landmarks)\n",
    "        \n",
    "        scaler.scale(loss_train_step).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # find the loss for the current step\n",
    "                \n",
    "        loss_train += loss_train_step\n",
    "        running_loss = loss_train/(step+1)\n",
    "        \n",
    "        description = f\"| # Train-Epoch : {epoch + 1} Loss : {(running_loss):.4f}\"\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pbar_valid = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "        for step, (images, landmarks) in pbar_valid:\n",
    "                  \n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).to(device)\n",
    "        \n",
    "            predictions = model(images).to(device)\n",
    "                    \n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step\n",
    "            running_loss = loss_valid/(step+1)\n",
    "\n",
    "            description = f\"| # Valid-Epoch : {epoch + 1} Loss : {(running_loss):.4f}\"\n",
    "            pbar_valid.set_description(description)\n",
    "            \n",
    "            \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(model.state_dict(), '/home/ubuntu/workspace/FLD-scratch/result/face_landmarks_kface.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, C.EPOCHS))\n",
    "        print('Model Saved\\n')\n",
    "\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    pretrained_model = '/home/ubuntu/workspace/FLD-scratch/result/face_landmarks_kface.pth'\n",
    "    model = resnet18().cpu()\n",
    "    if pretrained_model:\n",
    "        state_dict = torch.load(pretrained_model,map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    best_network = model.to(device)\n",
    "    \n",
    "    images, landmarks = next(iter(valid_loader))\n",
    "    \n",
    "    images = images.to(device)\n",
    "    landmarks *= 512 #* torch.tensor((512,512))    \n",
    "\n",
    "    predictions = best_network(images).cpu()\n",
    "    predictions = predictions.view([-1,27,2])\n",
    "    predictions *= 512# * torch.tensor((512,512))    \n",
    "    \n",
    "    plt.figure(figsize=(10,20))\n",
    "    \n",
    "    for img_num in range(2):\n",
    "        print(images[img_num].shape)\n",
    "        plt.subplot(2,1,img_num+1)\n",
    "        plt.imshow(images[img_num].cpu().permute(1,2,0).squeeze(), cmap='gray')\n",
    "        plt.scatter(predictions[img_num].T[0], predictions[img_num].T[1], c = 'r', s = 5)\n",
    "        plt.scatter(landmarks[img_num].T[0], landmarks[img_num].T[1], c = 'g', s = 5)\n",
    "\n",
    "print('Total number of test images: {}'.format(len(valid_dataset)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed Time : {}\".format(end_time - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('FG-GLD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9550210be859de316fbaaac654d2fc4c7fb8b5570e54a571b0ab8123deb4b39d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
